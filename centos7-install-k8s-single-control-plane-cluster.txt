K8s Cluster Installation

We will use kubeadm for bootstrap K8s cluster also upgrade, downgrade things.

Each node/server/vm minimum have 2 cpu, 2 gb ram and proper disks. Set more to go safety.

We will install single control plane with flannel pod networking.
1 Master 2 Worker nodes will be.

I will not set hostname for master and workers

Advise to create docker,kubeadm,kubelet,kubectl installed Centos 7 template to provision master and workers.
Links for kubeadm https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/ .

I accept that we have 3 nodes with docker,kubeadm,kubelet,kubectl installed  and start to bootstrap K8s Clusters

Before start, short and quick info ; 

kube-apiserver : Heart of K8s, everybody will ask and get something from it,all components interact via API server, its know all objects.
etcd : Database or K/V store,all things stored in.
kubelet : Get commands and execure order 66 :)
kube-proxy : Run or each node, like router or traffic manager, its open the windows for container to the world.
kube-scheduler : Decide where container should provisoned or to be
kube-controller-manager : Monitor desigred state
??cloud-controller-manager : Cloud provider specific integration
coredns : Used for service discovery
pause : its container and hold network namespace.

https://www.slideshare.net/BobKillen/kubernetes-a-comprehensive-overview-updated


Service Ports and Firewall Settings

On Masters

# 6443 kube-apiserver
firewall-cmd --permanent --add-port=6443/tcp
# 2379-80 Etcd
firewall-cmd --permanent --add-port=2379-2380/tcp
# Kubelet
firewall-cmd --permanent --add-port=10250/tcp
# Kube-scheduler
firewall-cmd --permanent --add-port=10251/tcp
# Kube-controller-manager
firewall-cmd --permanent --add-port=10252/tcp
firewall-cmd --permanent --add-port=10255/tcp
#Reload rules ....
firewall-cmd --complete-reload

On workers

# Kubelet
firewall-cmd --permanent --add-port=10250/tcp
# NodePort Services
firewall-cmd --permanent --add-port=30000-32767/tcp
#Reload rules ....
firewall-cmd --complete-reload

https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#check-required-ports

First try to pull the required images 

#On Master execute command "kubeadm config images pull"

[root@k8snm01 ~]# kubeadm config images pull
W0220 15:43:08.392429    5956 validation.go:28] Cannot validate kube-proxy config - no validator is available
W0220 15:43:08.392582    5956 validation.go:28] Cannot validate kubelet config - no validator is available
[config/images] Pulled k8s.gcr.io/kube-apiserver:v1.17.3
[config/images] Pulled k8s.gcr.io/kube-controller-manager:v1.17.3
[config/images] Pulled k8s.gcr.io/kube-scheduler:v1.17.3
[config/images] Pulled k8s.gcr.io/kube-proxy:v1.17.3
[config/images] Pulled k8s.gcr.io/pause:3.1
[config/images] Pulled k8s.gcr.io/etcd:3.4.3-0
[config/images] Pulled k8s.gcr.io/coredns:1.6.5

Before init k8s Cluster ; 

With init we can provide --control-plane-endpoint with ip address, this is for when multiple master node will be, for this article not needed
With init if we will use flannel we have to specify --pod-network-cidr but this kind of things are depend CNI ( Container Network Interface)
With init --apiserver-advertise-address we can set different interface other then default gw one.

Before init k8s Cluster ;

Pod networks(Container Networks) should not overlap with any host network. To be sure use --pod-network-cidr

Lets start ; 

#On Master

kubeadm init --pod-network-cidr=10.244.0.0/16


Not the path of auth things ....

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Not the worker nodes join command and hash 

kubeadm join 10.110.55.1:6443 --token bowknc.nku4n65c5zccgiw5 \
    --discovery-token-ca-cert-hash sha256:12d61b0ea93c3b9323cafab16f4e748a0557b8ed78749d95866364a4028a21c1

First install flannel pod networing 

kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/2140ac876ef134e0ed5af15c65e414cf26827915/Documentation/kube-flannel.yml

Get it from https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/

[root@k8snm01 ~]# kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/2140ac876ef134e0ed5af15c65e414cf26827915/Documentation/kube-flannel.yml
podsecuritypolicy.policy/psp.flannel.unprivileged created
clusterrole.rbac.authorization.k8s.io/flannel created
clusterrolebinding.rbac.authorization.k8s.io/flannel created
serviceaccount/flannel created
configmap/kube-flannel-cfg created
daemonset.apps/kube-flannel-ds-amd64 created
daemonset.apps/kube-flannel-ds-arm64 created
daemonset.apps/kube-flannel-ds-arm created
daemonset.apps/kube-flannel-ds-ppc64le created
daemonset.apps/kube-flannel-ds-s390x created

Look nodes and pods on kube-system namespace 

[root@k8snm01 ~]# kubectl get pods --all-namespaces
NAMESPACE     NAME                              READY   STATUS    RESTARTS   AGE
kube-system   coredns-6955765f44-sspvk          0/1     Running   0          10m
kube-system   coredns-6955765f44-zrbvx          0/1     Running   0          10m
kube-system   etcd-k8snm01                      1/1     Running   0          10m
kube-system   kube-apiserver-k8snm01            1/1     Running   0          10m
kube-system   kube-controller-manager-k8snm01   1/1     Running   0          10m
kube-system   kube-flannel-ds-amd64-42x2m       1/1     Running   0          2m44s
kube-system   kube-proxy-8647w                  1/1     Running   0          10m
kube-system   kube-scheduler-k8snm01            1/1     Running   0          10m

[root@k8snm01 ~]# kubectl get nodes
NAME      STATUS   ROLES    AGE    VERSION
k8snm01   Ready    master   9m6s   v1.17.3

Then join workers 

[root@k8snw01 ~]# kubeadm join 10.110.55.1:6443 --token bowknc.nku4n65c5zccgiw5 \
>     --discovery-token-ca-cert-hash sha256:12d61b0ea93c3b9323cafab16f4e748a0557b8ed78749d95866364a4028a21c1
W0220 17:54:45.508524   11444 join.go:346] [preflight] WARNING: JoinControlPane.controlPlane settings will be ignored when control-plane flag is not set.
[preflight] Running pre-flight checks
	[WARNING IsDockerSystemdCheck]: detected "cgroupfs" as the Docker cgroup driver. The recommended driver is "systemd". Please follow the guide at https://kubernetes.io/docs/setup/cri/
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
[kubelet-start] Downloading configuration for the kubelet from the "kubelet-config-1.17" ConfigMap in the kube-system namespace
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.

[root@k8snw02 ~]# kubeadm join 10.110.55.1:6443 --token bowknc.nku4n65c5zccgiw5 \
>     --discovery-token-ca-cert-hash sha256:12d61b0ea93c3b9323cafab16f4e748a0557b8ed78749d95866364a4028a21c1
W0220 17:46:47.164575   11340 join.go:346] [preflight] WARNING: JoinControlPane.controlPlane settings will be ignored when control-plane flag is not set.
[preflight] Running pre-flight checks
	[WARNING IsDockerSystemdCheck]: detected "cgroupfs" as the Docker cgroup driver. The recommended driver is "systemd". Please follow the guide at https://kubernetes.io/docs/setup/cri/
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
[kubelet-start] Downloading configuration for the kubelet from the "kubelet-config-1.17" ConfigMap in the kube-system namespace
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.

[root@k8snm01 ~]# kubectl get nodes
NAME      STATUS   ROLES    AGE     VERSION
k8snm01   Ready    master   64m     v1.17.3
k8snw01   Ready    <none>   3m54s   v1.17.3
k8snw02   Ready    <none>   3m50s   v1.17.3


All good ...
